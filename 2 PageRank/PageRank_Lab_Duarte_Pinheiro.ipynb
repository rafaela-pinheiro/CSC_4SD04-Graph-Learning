{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Learning\n",
    "## Lab 2: PageRank\n",
    "\n",
    "In this lab, you will learn to compute, use and interpret various [PageRank](https://en.wikipedia.org/wiki/PageRank) scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word;\">\n",
    "<bold>Done in pair:</bold> \n",
    "<br>\n",
    "Rafaela de Carvalho Machado Pinheiro\n",
    "Bárbara Barsi Duarte Batista da Silva\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sknetwork.data import load_netset, linear_graph, miserables\n",
    "from sknetwork.linalg import normalize\n",
    "from sknetwork.ranking import PageRank\n",
    "from sknetwork.visualization import visualize_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on the following graphs (see the [NetSet](https://netset.telecom-paris.fr/) collection for details):\n",
    "* Openflights (graph)\n",
    "* WikiVitals (directed graph)\n",
    "* Cinema (bipartite graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openflights = load_netset('openflights')\n",
    "wikivitals = load_netset('wikivitals')\n",
    "cinema = load_netset('cinema')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graphs\n",
    "\n",
    "The PageRank corresponds to the stationary distribution of a random walk with restart probability $1-\\alpha$. Unless otherwise specified, we take the default value $\\alpha = 0.85$ and the restart probability distribution is uniform over the set of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = linear_graph(n, True)\n",
    "adjacency = dataset.adjacency\n",
    "position = dataset.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = visualize_graph(adjacency, position, names=np.arange(n))\n",
    "SVG(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* What are the two best ranked nodes? Try with different values of $\\alpha$ and interpret the results.\n",
    "* What is the exact PageRank vector when $\\alpha=1$ (no restarts)? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.85\n",
    "\n",
    "pagerank = PageRank(damping_factor=alpha, solver='lanczos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pagerank.fit_predict(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_test(adjacency, final_alpha = 0.9, alpha_step = 0.2):\n",
    "    \n",
    "    scores_array = []\n",
    "\n",
    "    alphas = np.arange(0.0, final_alpha, alpha_step)\n",
    "\n",
    "    for alpha in alphas:\n",
    "        pagerank = PageRank(damping_factor=alpha, solver='lanczos')\n",
    "        scores_array.append(pagerank.fit_predict(adjacency))\n",
    "    \n",
    "    return scores_array, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores_array, alphas):\n",
    "    scores = np.array(scores_array)\n",
    "    num_groups = len(scores)\n",
    "    num_nodes = scores.shape[1]\n",
    "\n",
    "    bar_width = 0.15\n",
    "    index = np.arange(num_nodes)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        offset = i * bar_width\n",
    "        ax.bar(index + offset, scores[i], bar_width, label=f'α = {alphas[i]:.2f}')\n",
    "\n",
    "    ax.set_xlabel('Nodes')\n",
    "    ax.set_xticks(index + bar_width * (num_groups - 1) / 2) \n",
    "    ax.set_xticklabels([f'{i}' for i in range(num_nodes)]) \n",
    "    ax.set_ylabel('Scores (PageRank)')\n",
    "    ax.set_title('Node scores for different values of α')\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = visualize_graph(adjacency, position, names=np.arange(n), scores=scores)\n",
    "SVG(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_array, alphas = scores_test(adjacency)\n",
    "plot_scores(scores_array, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<!DOCTYPE math PUBLIC \"-//W3C//DTD MathML 2.0//EN\" \"http://www.w3.org/Math/DTD/mathml2/mathml2.dtd\">\n",
    "\n",
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word;\">\n",
    "\n",
    "The two best ranked nodes are 1 and 8, since they are the only ones connected to nodes with no other connection except for these two nodes (0 is connected only to 1, and 9 is connected only to 8). The patern of the score increasing in case of the alpha value increase can be noticed at the graph above, that ilustrate the score for each node for values of alpha going from 0 to 0.8.\n",
    "\n",
    " With a low damping factor (α ≈ 0.2), isolated nodes (0 and 9) lose importance since the algorithm relies more on link structure. With a higher factor (α ≈ 0.8), scores balance out due to random teleportation, redistributing weight more evenly. \n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the PageRank only acept a damping factor value in [0, 1[ , we can use a value closer to 1, like 0.99,\n",
    "# to simulate the behavior of a random walker that does not restart to a random page.\n",
    "\n",
    "alpha = 0.9999\n",
    "pagerank = PageRank(damping_factor=alpha, solver='lanczos')\n",
    "scores = pagerank.fit_predict(adjacency)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word; \">\n",
    "    When the damping factor <math xmlns=\"http://www.w3.org/1998/Math/MathML\"> <mi>α</mi></math> is equal to 1, the equation \n",
    "\t<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "\t<mrow><msup><mi>π</mi><mrow><mo>(</mo><mi>α</mi><mo>)</mo></mrow></msup><mo>=</mo><mi>α</mi><msup><mi>π</mi><mrow><mo>(</mo><mi>α</mi><mo>)</mo></mrow></msup><mi>P</mi><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo>)</mo><mfrac><msup>\t<mn>1</mn>\t<mi>T</mi></msup><mi>n</mi></mfrac></mrow>\n",
    "\t</math>\n",
    "\treduces to the standard left eigenvector equation\n",
    "\t<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "\t<mrow><msup><mi>π</mi><mrow><mo>(</mo><mi>α</mi><mo>)</mo></mrow></msup><mo>=</mo><msup><mi>π</mi><mrow><mo>(</mo><mi>α</mi><mo>)</mo></mrow></msup><mi>P</mi>\n",
    "\t</math>. \n",
    "\t</br></br>\n",
    "\tThis means the PageRank vector \n",
    "\t<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "\t<mi>π</mi>\n",
    "\t</math>\n",
    "\tcorresponds exactly to the stationary distribution of the transition matrix \\(P\\), and can be interpreted as the left eigenvector associated with eigenvalue 1. In this case, there are no random jumps or restarts in the Markov process—the user simply follows the existing links indefinitely, reflecting only the structure of the underlying graph.\n",
    "\t</br></br>\n",
    "\tAs a result, the PageRank vector assigns more weight to central nodes and less to the border ones. Therefore, we can affirm that final vector\n",
    "\t</br></br>\n",
    "\t<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "\t<mi>P</mi>\n",
    "\t<mo>=</mo>\n",
    "\t<mrow>\n",
    "\t\t<mtable>\n",
    "\t\t<mtr><mtd><mn>0.0555563</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.1111117</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.11111104</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.11111059</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.11111037</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.11111037</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.11111059</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.11111104</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.1111117</mn></mtd></mtr>\n",
    "\t\t<mtr><mtd><mn>0.0555563</mn></mtd></mtr>\n",
    "\t\t</mtable>\n",
    "\t</mrow>\n",
    "\t</math>\n",
    "\t</br></br>\n",
    "\tis indeed the left eigenvector of the transition matrix P, obtained by the exact PageRank distribution when \n",
    "\t<math xmlns=\"http://www.w3.org/1998/Math/MathML\"> <mi>α</mi><mo>=</mo><mo>1</mo></math>.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les Misérables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetLM = miserables(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencyLM = datasetLM.adjacency\n",
    "positionLM = datasetLM.position\n",
    "namesLM = datasetLM.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = visualize_graph(adjacencyLM, positionLM, namesLM, scale=2)\n",
    "SVG(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Display the graph of Les Misérables with PageRank scores.\n",
    "* List the 10 best ranked characters.\n",
    "* Compare with:\n",
    "    1. the 10 nodes of highest degrees\n",
    "    2. the 10 nodes of highest weights\n",
    "* Try different values of $\\alpha$ and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerankLM = PageRank(damping_factor=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresLM = pagerank.fit_predict(adjacencyLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores in log scale appear more clearly\n",
    "imageLM = visualize_graph(adjacencyLM, positionLM, namesLM, scores=np.log(scoresLM), scale=2)\n",
    "SVG(imageLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sknetwork.utils import get_degrees, get_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 best ranked nodes\n",
    "ranked_indices = np.argsort(scoresLM)[-10:][::-1]\n",
    "\n",
    "for rank, i in enumerate(ranked_indices, 1):\n",
    "    print(f\"{rank}. {datasetLM.names[i]} (score: {scoresLM[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 weightes\n",
    "weights = get_weights(adjacencyLM)\n",
    "weights_indices = np.argsort(weights)[-10:][::-1] \n",
    "for rank, i in enumerate(weights_indices, 1):\n",
    "    print(f\"{rank}. {datasetLM.names[i]} (weight: {weights[i]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 degrees\n",
    "degrees = get_degrees(adjacencyLM)\n",
    "\n",
    "degrees_indices = np.argsort(degrees)[-10:][::-1]\n",
    "\n",
    "for rank, i in enumerate(degrees_indices, 1):\n",
    "    print(f\"{rank}. {datasetLM.names[i]} (degree: {degrees[i]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for different values of alpha\n",
    "scores_array, alphas = scores_test(adjacencyLM)\n",
    "\n",
    "for i in range(len(scores_array)):\n",
    "\n",
    "    print(f\"\\nAlpha = {alphas[i]:.2f}\")\n",
    "\n",
    "    print(\"\\n Top 5 best ranked nodes\")\n",
    "\n",
    "    # Top 10 best ranked nodes\n",
    "    ranked_indices = np.argsort(scoresLM)[-5:][::-1]\n",
    "\n",
    "    for rank, i in enumerate(ranked_indices, 1):\n",
    "        print(f\"{rank}. {datasetLM.names[i]} (score: {scoresLM[i]:.4f})\")\n",
    "\n",
    "    print(\"\\n Top 5 weightes\")\n",
    "\n",
    "    # Top 10 weightes\n",
    "    weights = get_weights(adjacencyLM)\n",
    "    weights_indices = np.argsort(weights)[-5:][::-1] \n",
    "    for rank, i in enumerate(weights_indices, 1):\n",
    "        print(f\"{rank}. {datasetLM.names[i]} (weight: {weights[i]:.2f})\")\n",
    "\n",
    "    print(\"\\n Top 5 degrees\")\n",
    "    \n",
    "    # Top 10 degrees\n",
    "    degrees = get_degrees(adjacencyLM)\n",
    "\n",
    "    degrees_indices = np.argsort(degrees)[-5:][::-1]\n",
    "\n",
    "    for rank, i in enumerate(degrees_indices, 1):\n",
    "        print(f\"{rank}. {datasetLM.names[i]} (degree: {degrees[i]:.2f})\")\n",
    "        \n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openflights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetOF = openflights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencyOF = datasetOF.adjacency\n",
    "positionOF = datasetOF.position\n",
    "namesOF = datasetOF.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide the edges for better visualization\n",
    "image = visualize_graph(adjacencyOF, positionOF, width=800, height=400, display_node_weight=True, display_edges=False)\n",
    "SVG(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Display the same world map with PageRank scores (in log scale).\n",
    "* List the 10 best ranked airports, and compare with the 10 airports of highest traffic.\n",
    "* Display the world map with Personalized PageRank scores, starting from Tokyo international airport.\n",
    "* List the corresponding 10 best ranked airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerankOF = PageRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresOF = pagerankOF.fit_predict(adjacencyOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = visualize_graph(adjacencyOF, positionOF, scores=np.log(scoresOF), node_order=np.argsort(scoresOF), \n",
    "                  width=800, height=400, display_node_weight=True, display_edges=False)\n",
    "SVG(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 best ranked nodes\n",
    "ranked_indices = np.argsort(scoresOF)[-10:][::-1]\n",
    "for rank, i in enumerate(ranked_indices, 1):\n",
    "    print(f\"{rank}. {datasetOF.names[i]} (score: {scoresOF[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 airports by traffic\n",
    "weights = get_weights(adjacencyOF)\n",
    "weights_indices = np.argsort(weights)[-10:][::-1]\n",
    "for rank, i in enumerate(weights_indices, 1):\n",
    "    print(f\"{rank}. {datasetOF.names[i]} (weight: {weights[i]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word;\">\n",
    "Although there are 5 airports that appear in both lists, being well ranked and having high traffic do not mean the same. The best ranked airports are the 10 most important for the connectivity of the graph, as they received/send flights from/to other important airports. However, the high traffic (high weight) airports are high-volume hubs, regardless of how important their connections are.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Tokyo airport index\n",
    "tokyo_index = [i for i, name in enumerate(namesOF) if 'tokyo' in name.lower()]\n",
    "print(f\"Index: {tokyo_index}, Name: {namesOF[tokyo_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tokyo_index:\n",
    "    tokyo_index = int(i)\n",
    "perso_scores = pagerank.fit_predict(adjacencyOF, {tokyo_index: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = visualize_graph(adjacencyOF, positionOF, scores=np.log(perso_scores), node_order=np.argsort(perso_scores), \n",
    "                  width=800, height=400, display_node_weight=True, display_edges=False)\n",
    "SVG(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 best ranked nodes with personalized scores\n",
    "ranked_indices = np.argsort(perso_scores)[-11:][::-1]\n",
    "for rank, i in enumerate(ranked_indices, 1):\n",
    "    print(f\"{rank}. {datasetOF.names[i]} (score: {perso_scores[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Directed graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetWV = wikivitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencyWV = datasetWV.adjacency\n",
    "namesWV = datasetWV.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* List the 10 best ranked articles of Wikipedia Vitals.\n",
    "* Compare with the 10 nodes of highest out-degrees and the 10 nodes of highest in-degrees. Interpret the results.\n",
    "* Which article of Wikipedia Vitals is in the top-20 of PageRank but not in the top-20 of in-degrees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 best ranked nodes\n",
    "pagerankWV = PageRank()\n",
    "scoresWV = pagerankWV.fit_predict(adjacencyWV)\n",
    "\n",
    "ranked_indices = np.argsort(scoresWV)[-10:][::-1]\n",
    "for rank, i in enumerate(ranked_indices, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (score: {scoresWV[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The out-degree of a node i is defined as A * 1_i, while the in-degree is defined similarly A^T * 1_i. \n",
    "# To get those values, we can use the get_degrees function from sknetwork.utils, only specifying the transpose parameter to True for the in-degree.\n",
    "\n",
    "out_degrees = get_degrees(adjacencyWV)\n",
    "in_degrees = get_degrees(adjacencyWV, transpose=True)\n",
    "\n",
    "out_degrees_indices = np.argsort(out_degrees)[-10:][::-1]\n",
    "print(\"\\nTop 10 nodes of highest out-degree\")\n",
    "for rank, i in enumerate(out_degrees_indices, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (out-degree: {out_degrees[i]:.2f})\")\n",
    "\n",
    "print(\"\\nTop 10 nodes of highest in-degree\")\n",
    "in_degrees_indices = np.argsort(in_degrees)[-10:][::-1]\n",
    "for rank, i in enumerate(in_degrees_indices, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (out-degree: {in_degrees[i]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word;\">\n",
    "Nodes with high out-degree values indicate that those pages contain more hyperlinks to other pages on Wikipedia, with their content being more general or not specific to a particular topic (which leads to more references to external pages and articles). In the other hand, nodes with high in-degree values indicate that those pages are more frequently linked to by other pages, being metioned in other articles due to their importance or relevance in diverse contexts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 best ranked articles\n",
    "ranked_indices = np.argsort(scoresWV)[-20:][::-1]\n",
    "scores_list = [datasetWV.names[i] for i in ranked_indices]\n",
    "\n",
    "in_degrees_indices = np.argsort(in_degrees)[-20:][::-1]\n",
    "in_degrees_list = [datasetWV.names[i] for i in in_degrees_indices]\n",
    "\n",
    "print(\"Page present in top 20 hightest ranked articles but not in the top 20 highest in-degree articles\")\n",
    "print(set(scores_list) - set(in_degrees_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* List the 20 closest articles to **Picasso** in Wikipedia Vitals. Who is the best ranked painter other than Picasso?\n",
    "* List the 20 closest articles to both **Cat** and **Dog** in Wikipedia Vitals.\n",
    "* In both cases, do the same using the difference between the Personalized PageRank score and the PageRank score. Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Picasso index\n",
    "picasso_index = [i for i, name in enumerate(namesWV) if 'pablo picasso' in name.lower()]\n",
    "print(f\"Index: {picasso_index}, Name: {namesWV[picasso_index]}\")\n",
    "\n",
    "pp_perso_scores = pagerankWV.fit_predict(adjacencyWV, {picasso_index[0]: 1})\n",
    "ranked_indices_pp_perso = np.argsort(pp_perso_scores)[-20:][::-1]\n",
    "\n",
    "print(\"\\nTop 20 closest articles to Pablo Picasso\")\n",
    "for rank, i in enumerate(ranked_indices_pp_perso, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (score: {pp_perso_scores[i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_index = [i for i, name in enumerate(namesWV) if 'dog' == name.lower()]\n",
    "print(f\"Index: {dog_index}, Name: {namesWV[dog_index]}\")\n",
    "\n",
    "dog_perso_scores = pagerankWV.fit_predict(adjacencyWV, {dog_index[0]: 1})\n",
    "ranked_indices_dog = np.argsort(dog_perso_scores)[-20:][::-1]\n",
    "\n",
    "print(\"\\nTop 20 closest articles to dog\\n\")\n",
    "for rank, i in enumerate(ranked_indices_dog, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (score: {dog_perso_scores[i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 scores for cat and dog\n",
    "\n",
    "dog_index = [i for i, name in enumerate(namesWV) if 'dog' == name.lower()]\n",
    "cat_index = [i for i, name in enumerate(namesWV) if 'cat' == name.lower()]\n",
    "print(f\"Index: {dog_index}, Name: {namesWV[dog_index]}\")\n",
    "print(f\"Index: {cat_index}, Name: {namesWV[cat_index]}\")\n",
    "\n",
    "cd_perso_scores = pagerankWV.fit_predict(adjacencyWV, {dog_index[0]: 1, cat_index[0]: 1})\n",
    "ranked_indices_cd = np.argsort(cd_perso_scores)[-20:][::-1]\n",
    "\n",
    "print(\"\\n Top 20 closest articles to cat and dog\\n\")\n",
    "for rank, i in enumerate(ranked_indices_cd, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (score: {cd_perso_scores[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dif_pp = pp_perso_scores - scoresWV\n",
    "ranked_indices_pp_dif = np.argsort(scores_dif_pp)[-20:][::-1]\n",
    "\n",
    "print(\"Top 20 closest articles to Pablo Picasso (using difference between personalized and standard page rank\\n\")\n",
    "\n",
    "for rank, i in enumerate(ranked_indices_pp_dif, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (score: {scores_dif_pp[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dif_cd = cd_perso_scores - scoresWV\n",
    "ranked_indices_cd_dif =  np.argsort(scores_dif_cd)[-20:][::-1]\n",
    "\n",
    "print(\"Top 20 closest articles to both cat and dog (using difference between personalized and standard pagerank)\\n\")\n",
    "for rank, i in enumerate(ranked_indices_cd_dif, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (score: {cd_perso_scores[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word;\">\n",
    "The result obtained by using certain nodes as the only non-zero elements in the teleportation vector produces a more accurate result towards the articles related to these topics.  However, globally high-ranking nodes can still influence the Personalized PageRank results even when they are not included in the teleportation vector. Since they have a high number of connections, their scores are still high, even when we observe specifically the results biased on the teleportation vector.\n",
    "\n",
    "To avoid this, we can remove this bias by subtracting the standard PageRank scores from the personalized PageRank scores, which will give us a clearer view of the articles that are more closely related to the topics we are interested in.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* List 5 representative articles of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 11 categories\n",
    "labels = datasetWV.labels\n",
    "names_labels = datasetWV.names_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word;\">\n",
    "To identify representative articles for each category, we compute a Personalized PageRank with the restart distribution (weights) biased toward nodes belonging to a given category.\n",
    "\n",
    "The resulting PageRank scores reflect the influence of nodes under the structural context of the entire graph but with focus on the target category. After computing the scores, we filter the result to retain only the nodes that belong to the label l, ensuring that the top-ranked nodes are consistent with their category.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = { l: [] for l in names_labels}\n",
    "\n",
    "for i, label in enumerate(names_labels, 0):\n",
    "    selected_pages = { p: 1 for p in np.where(labels == i)[0]}\n",
    "    label_scores = pagerankWV.fit_predict(adjacencyWV, weights = selected_pages)\n",
    "\n",
    "    ranked_indices_label = np.argsort(-label_scores)\n",
    "    for p in ranked_indices_label:\n",
    "        if p in selected_pages.keys():\n",
    "            labels_dict[label].append(namesWV[p])\n",
    "        if len(labels_dict[label]) > 5:\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"\\nTop 5 articles for each category:\\n\")\n",
    "for label, pages in labels_dict.items():\n",
    "    print(f\"{label}: { \", \".join(pages)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bipartite graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetC = cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biadjacency = datasetC.biadjacency\n",
    "movies = datasetC.names_row\n",
    "actors = datasetC.names_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "List the 5 closest actors and the 5 closest movies to **Catherine Deneuve**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word;\">\n",
    "To do this, we can use a personalized PageRank again, to start from Catherine Deneuve and then find the 5 closest movies and actors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for Catherine Deneuve\n",
    "index = [i for i, name in enumerate(actors) if 'catherine deneuve' in name.lower()]\n",
    "catherine_index = int(index[0])\n",
    "print(f\"Index: {catherine_index}, Name: {actors[catherine_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresC = pagerank.fit_predict(biadjacency, weights_col = {catherine_index: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_actors = pagerank.scores_col_\n",
    "scores_movies = pagerank.scores_row_ # same as scoresC\n",
    "\n",
    "print(\"5 closest movies:\")\n",
    "ranked_indices = np.argsort(scores_movies)[-5:][::-1]\n",
    "for rank, i in enumerate(ranked_indices, 1):\n",
    "    print(f\"{rank}. {movies[i]} (score: {scores_movies[i]:.4f})\")\n",
    "\n",
    "print(\"\\n5 closest actors:\")\n",
    "ranked_indices = np.argsort(scores_actors)[-6:][::-1]\n",
    "for rank, i in enumerate(ranked_indices, 1):\n",
    "    print(f\"{rank}. {actors[i]} (score: {scores_actors[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Directed graphs as bipartite graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directed graphs can be represented as bipartite graphs by duplicating each node, one as source of edges and the other as destination of edges. The biadjacency matrix of the bipartite graph is simply the adjacency matrix of the directed graph. \n",
    "\n",
    "The PageRank scores obtained with the bipartite graph differ from those obtained with the directed graph: they correspond to the **forward-backward** random walk in the directed graph, edges being alternately followed in forward and backward directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetWV_bi = wikivitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencyWV_bi = datasetWV.adjacency\n",
    "namesWV_bi = datasetWV.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "Do the same experiments as above and compare both rankings:\n",
    "* List the 10 best ranked articles of Wikipedia Vitals. \n",
    "* List the 20 closest articles to **Picasso** in Wikipedia Vitals. \n",
    "* List the 20 closest articles to both **Cat** and **Dog** in Wikipedia Vitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresWV_bi = pagerank.fit_predict(adjacencyWV_bi, force_bipartite=True)\n",
    "ranked_indices_bi = np.argsort(scoresWV_bi)[-10:][::-1]\n",
    "\n",
    "for rank, i in enumerate(ranked_indices_bi, 1):\n",
    "    print(f\"{rank}. {datasetWV.names[i]} (score: {scoresWV_bi[i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Picasso index\n",
    "\n",
    "picasso_index = [i for i, name in enumerate(namesWV_bi) if 'pablo picasso' in name.lower()]\n",
    "print(f\"Index: {picasso_index}, Name: {namesWV_bi[picasso_index]}\")\n",
    "\n",
    "pp_scores_bi = pagerankWV.fit_predict(adjacencyWV_bi, {picasso_index[0]: 1})\n",
    "ranked_indices_pp_bi = np.argsort(pp_scores_bi)[-20:][::-1]\n",
    "\n",
    "print(\"\\nTop 20 closest articles to Pablo Picasso\")\n",
    "for rank, i in enumerate(ranked_indices_pp_bi, 1):\n",
    "    print(f\"{rank}. {datasetWV_bi.names[i]} (score: {pp_scores_bi[i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 scores for cat and dog\n",
    "\n",
    "dog_index = [i for i, name in enumerate(namesWV_bi) if 'dog' == name.lower()]\n",
    "cat_index = [i for i, name in enumerate(namesWV_bi) if 'cat' == name.lower()]\n",
    "\n",
    "print(f\"Index: {dog_index}, Name: {namesWV_bi[dog_index]}\")\n",
    "print(f\"Index: {cat_index}, Name: {namesWV_bi[cat_index]}\")\n",
    "\n",
    "cd_scores_bi = pagerankWV.fit_predict(adjacencyWV, {dog_index[0]: 1, cat_index[0]: 1})\n",
    "ranked_indices_cd_bi = np.argsort(cd_scores_bi)[-20:][::-1]\n",
    "\n",
    "print(\"\\n Top 20 closest articles to cat and dog\\n\")\n",
    "for rank, i in enumerate(ranked_indices_cd_bi, 1):\n",
    "    print(f\"{rank}. {datasetWV_bi.names[i]} (score: {cd_scores_bi[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 10px; display: inline-block; max-width: 98%; box-sizing: border-box; word-wrap: break-word;\">\n",
    "When comparing the results from standard or personalized PageRank to those obtained by enforcing a bipartition of the graph, we observe a fundamental shift in the nature of the\n",
    "ranking. In a standard graph structure, PageRank leverages the full connectivity of the network, allowing influence to propagate freely across all nodes.\n",
    "</br>\n",
    "When we force a bipartition to the PageRank, each node’s influence must pass through the opposing set. As a result, the nodes that emerge as important under bipartition tend to be\n",
    " those that act as strong bridges or connectors between the two partitions, rather than those that are globally influential. This modification highlights nodes that are structurally critical to inter-group communication, and change mainly the result of the general rank of the Wikipedia Vitals dataset.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
